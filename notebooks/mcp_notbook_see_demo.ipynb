{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ff44e18-d9d4-4f3b-a9b4-8ae82ca3879e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://ollama-service-internal.default.svc.cluster.local:11433\n",
      "model: llama3.2:1b\n",
      "embedding: bge-m3:567m\n",
      "datapath /app/data\n",
      "no_proxy None\n",
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "OLLAMA_URL=os.getenv(\"OLLAMA_URL\")\n",
    "LLM_MODEL=os.getenv(\"LLM_MODEL\")\n",
    "EMBEDDING_MODEL=os.getenv(\"EMBEDDING_MODEL\")\n",
    "DATAPATH=os.getenv(\"DATAPATH\")\n",
    "#OLLAMA_URL='http://localhost:11434'\n",
    "LLM_MODEL=\"llama3.2:1b\"\n",
    "#LLM_MODEL=\"llama3.2:3b\" # max retires\n",
    "#LLM_MODEL=\"gemma3:4b\" # no tool support\n",
    "#LLM_MODEL=\"llava:7b\"\n",
    "#LLM_MODEL=\"codellama:7b\"\n",
    "#LLM_MODEL=\"qwen2.5-coder:1.5b\" # no tool support\n",
    "#LLM_MODEL=\"deepseek-r1:1.5b\" # does not support tools\n",
    "#os.setenv(\"no_proxy=localhost,127.0.0.1,*.my-it-solutions.net,ollama\")\n",
    "\n",
    "print(OLLAMA_URL)\n",
    "print(f\"model: {LLM_MODEL}\")\n",
    "print(f\"embedding: {EMBEDDING_MODEL}\")\n",
    "print(f\"datapath {DATAPATH}\")\n",
    "print(f\"no_proxy {os.getenv('no_proxy')}\")\n",
    "\n",
    "import requests\n",
    "import time\n",
    "\n",
    "response = requests.get(OLLAMA_URL,timeout=3)\n",
    "if response.status_code == requests.codes.ok:\n",
    "    print(response)\n",
    "else:\n",
    "    print(response)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28ec15ba-2599-4928-b321-2d9139984960",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from llama_index.llms.ollama import Ollama\n",
    "from pydantic_ai.models.openai import OpenAIModel\n",
    "from pydantic_ai.providers.openai import OpenAIProvider\n",
    "#llm = Ollama(model=LLM_MODEL, request_timeout=120.0, base_url=OLLAMA_URL, temperature=0.0)\n",
    "\n",
    "\n",
    "ollama_model = OpenAIModel(\n",
    "    model_name='LLM_MODEL', provider=OpenAIProvider(base_url=f\"{OLLAMA_URL}/v1\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bcadbd6-6a2d-4e55-af36-2693f9603691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [1667]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:55168 - \"GET /sse HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "from starlette.applications import Starlette\n",
    "from starlette.routing import Mount, Host\n",
    "import uvicorn\n",
    "\n",
    "from pydantic_ai import Agent\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "#loop = asyncio.get_event_loop()\n",
    "\n",
    "\n",
    "server = FastMCP('PydanticAI Server')\n",
    "server_agent = Agent(model=ollama_model,\n",
    "              result_type=str,\n",
    "              deps_type=str,\n",
    "              system_prompt=\"\",              \n",
    "              retries=2)\n",
    "\n",
    "@server.tool()\n",
    "async def poet(theme: str) -> str:\n",
    "    \"\"\"Poem generator\"\"\"\n",
    "    print(\"test\")\n",
    "    r = await server_agent.run(f'write a poem about {theme}')    \n",
    "    return r.data\n",
    "\n",
    "@server.prompt()\n",
    "def review_code(code: str) -> str:\n",
    "    return f\"Please review this code:\\n\\n{code}\"    \n",
    "\n",
    "\n",
    "#server.run()\n",
    "#asyncio.run_coroutine_threadsafe(server.run('sse'), loop)\n",
    "#asyncio.run(server.run('sse'))\n",
    "# Mount the SSE server to the existing ASGI server\n",
    "app = Starlette(\n",
    "    routes=[\n",
    "        Mount('/', app=server.sse_app()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# or dynamically mount as host\n",
    "app.router.routes.append(Host('localhost', app=server.sse_app()))\n",
    "\n",
    "#LLM_MODEL=\"llama3.2:3b.run(app, host=\"127.0.0.1\", port=8000)\n",
    "bg = None\n",
    "async def start_mcp_server(bg):\n",
    "    bg = await uvicorn.run(app, host='0.0.0.0', port=8001, log_level=\"info\")\n",
    "    #config = uvicorn.Config(\"notebook:app\", host='0.0.0.0', port=8001, log_level=\"info\")\n",
    "    #server = uvicorn.Server(config)\n",
    "    #await server.serve()\n",
    "\n",
    "if bg is not None:\n",
    "    print(\"alrady runnign, stop\")\n",
    "    bg.stop()\n",
    "\n",
    "asyncio.create_task(start_mcp_server(bg))\n",
    "#start_mcp_server(bg)\n",
    "print(\"started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "886b7514-1357-4442-a43f-b2dc2a66cfd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9177f66-b412-4bfb-9214-5f861ea88a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/27/25 23:38:59] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Connecting to SSE endpoint: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://localhost:8001/sse</span>                        <a href=\"file:///opt/venv/lib/python3.13/site-packages/mcp/client/sse.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sse.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/venv/lib/python3.13/site-packages/mcp/client/sse.py#45\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">45</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/27/25 23:38:59]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Connecting to SSE endpoint: \u001b[4;94mhttp://localhost:8001/sse\u001b[0m                        \u001b]8;id=441288;file:///opt/venv/lib/python3.13/site-packages/mcp/client/sse.py\u001b\\\u001b[2msse.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=691221;file:///opt/venv/lib/python3.13/site-packages/mcp/client/sse.py#45\u001b\\\u001b[2m45\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">GET</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://localhost:8001/sse</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>          <a href=\"file:///opt/venv/lib/python3.13/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/venv/lib/python3.13/site-packages/httpx/_client.py#1786\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1786</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mGET\u001b[0m \u001b[4;94mhttp://localhost:8001/sse\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m          \u001b]8;id=568873;file:///opt/venv/lib/python3.13/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=635721;file:///opt/venv/lib/python3.13/site-packages/httpx/_client.py#1786\u001b\\\u001b[2m1786\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.mcp import MCPServerHTTP\n",
    "\n",
    "server = MCPServerHTTP(url='http://localhost:8001/sse')  \n",
    "agent = Agent(model=ollama_model,\n",
    "              result_type=str,\n",
    "              deps_type=str,\n",
    "              system_prompt=\"\",\n",
    "              mcp_servers=[server],\n",
    "              retries=1)\n",
    "\n",
    "\n",
    "async def main():\n",
    "    async with agent.run_mcp_servers():  \n",
    "        result = await agent.run('Schreibe ein Gedicht über Füsse')\n",
    "    print(result.data)\n",
    "    #> There are 9,208 days between January 1, 2000, and March 18, 2025.\n",
    "\n",
    "result = await main()\n",
    "print(result.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3ff79d-a208-48b4-a4b5-f48dbbb6b5f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
