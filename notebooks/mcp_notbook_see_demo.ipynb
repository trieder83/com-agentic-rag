{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ff44e18-d9d4-4f3b-a9b4-8ae82ca3879e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://ollama-service-internal.default.svc.cluster.local:11433\n",
      "model: llama3.2:3b\n",
      "embedding: bge-m3:567m\n",
      "datapath /app/data\n",
      "no_proxy None\n",
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "OLLAMA_URL=os.getenv(\"OLLAMA_URL\")\n",
    "LLM_MODEL=os.getenv(\"LLM_MODEL\")\n",
    "EMBEDDING_MODEL=os.getenv(\"EMBEDDING_MODEL\")\n",
    "DATAPATH=os.getenv(\"DATAPATH\")\n",
    "#OLLAMA_URL='http://localhost:11434'\n",
    "#LLM_MODEL=\"llama3.2:3b\" # max retires\n",
    "#LLM_MODEL=\"gemma3:4b\" # no tool support\n",
    "#LLM_MODEL=\"llava:7b\"\n",
    "#LLM_MODEL=\"codellama:7b\"\n",
    "#LLM_MODEL=\"qwen2.5-coder:1.5b\" # no tool support\n",
    "#LLM_MODEL=\"deepseek-r1:1.5b\" # does not support tools\n",
    "#os.setenv(\"no_proxy=localhost,127.0.0.1,*.my-it-solutions.net,ollama\")\n",
    "\n",
    "print(OLLAMA_URL)\n",
    "print(f\"model: {LLM_MODEL}\")\n",
    "print(f\"embedding: {EMBEDDING_MODEL}\")\n",
    "print(f\"datapath {DATAPATH}\")\n",
    "print(f\"no_proxy {os.getenv('no_proxy')}\")\n",
    "\n",
    "import requests\n",
    "import time\n",
    "\n",
    "response = requests.get(OLLAMA_URL,timeout=3)\n",
    "if response.status_code == requests.codes.ok:\n",
    "    print(response)\n",
    "else:\n",
    "    print(response)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28ec15ba-2599-4928-b321-2d9139984960",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from llama_index.llms.ollama import Ollama\n",
    "from pydantic_ai.models.openai import OpenAIModel\n",
    "from pydantic_ai.providers.openai import OpenAIProvider\n",
    "#llm = Ollama(model=LLM_MODEL, request_timeout=120.0, base_url=OLLAMA_URL, temperature=0.0)\n",
    "\n",
    "\n",
    "ollama_model = OpenAIModel(\n",
    "    model_name='LLM_MODEL', provider=OpenAIProvider(base_url=f\"{OLLAMA_URL}/v1\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bcadbd6-6a2d-4e55-af36-2693f9603691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "from starlette.applications import Starlette\n",
    "from starlette.routing import Mount, Host\n",
    "\n",
    "from pydantic_ai import Agent\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "#loop = asyncio.get_event_loop()\n",
    "\n",
    "\n",
    "server = FastMCP('PydanticAI Server')\n",
    "server_agent = Agent(model=ollama_model,\n",
    "              result_type=str,\n",
    "              deps_type=str,\n",
    "              system_prompt=\"\",              \n",
    "              retries=2)\n",
    "\n",
    "@server.tool()\n",
    "async def poet(theme: str) -> str:\n",
    "    \"\"\"Poem generator\"\"\"\n",
    "    r = await server_agent.run(f'write a poem about {theme}')\n",
    "    return r.data\n",
    "\n",
    "@server.prompt()\n",
    "def review_code(code: str) -> str:\n",
    "    return f\"Please review this code:\\n\\n{code}\"    \n",
    "\n",
    "\n",
    "#server.run()\n",
    "#asyncio.run_coroutine_threadsafe(server.run('sse'), loop)\n",
    "#asyncio.run(server.run('sse'))\n",
    "# Mount the SSE server to the existing ASGI server\n",
    "app = Starlette(\n",
    "    routes=[\n",
    "        Mount('/', app=server.sse_app()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# or dynamically mount as host\n",
    "app.router.routes.append(Host('localhost', app=server.sse_app()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "886b7514-1357-4442-a43f-b2dc2a66cfd8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fastmcp'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfastmcp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastMCP\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msqlite3\u001b[39;00m\n\u001b[32m      4\u001b[39m mcp = FastMCP(\u001b[33m\"\u001b[39m\u001b[33mSQLite Explorer\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'fastmcp'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9177f66-b412-4bfb-9214-5f861ea88a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/27/25 23:06:25] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Connecting to SSE endpoint: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://localhost:8888/sse</span>                        <a href=\"file:///opt/venv/lib/python3.13/site-packages/mcp/client/sse.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sse.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/venv/lib/python3.13/site-packages/mcp/client/sse.py#45\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">45</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/27/25 23:06:25]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Connecting to SSE endpoint: \u001b[4;94mhttp://localhost:8888/sse\u001b[0m                        \u001b]8;id=718312;file:///opt/venv/lib/python3.13/site-packages/mcp/client/sse.py\u001b\\\u001b[2msse.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=871641;file:///opt/venv/lib/python3.13/site-packages/mcp/client/sse.py#45\u001b\\\u001b[2m45\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">GET</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://localhost:8888/sse</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 404 Not Found\"</span>   <a href=\"file:///opt/venv/lib/python3.13/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/venv/lib/python3.13/site-packages/httpx/_client.py#1786\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1786</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mGET\u001b[0m \u001b[4;94mhttp://localhost:8888/sse\u001b[0m \u001b[32m\"HTTP/1.1 404 Not Found\"\u001b[0m   \u001b]8;id=796581;file:///opt/venv/lib/python3.13/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=998389;file:///opt/venv/lib/python3.13/site-packages/httpx/_client.py#1786\u001b\\\u001b[2m1786\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  + Exception Group Traceback (most recent call last):\n",
      "  |   File \"/opt/venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py\", line 3547, in run_code\n",
      "  |     await eval(code_obj, self.user_global_ns, self.user_ns)\n",
      "  |   File \"/tmp/ipykernel_858/880284894.py\", line 19, in <module>\n",
      "  |     result = await main()\n",
      "  |              ^^^^^^^^^^^^\n",
      "  |   File \"/tmp/ipykernel_858/880284894.py\", line 14, in main\n",
      "  |     async with agent.run_mcp_servers():\n",
      "  |                ~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  |   File \"/usr/local/lib/python3.13/contextlib.py\", line 214, in __aenter__\n",
      "  |     return await anext(self.gen)\n",
      "  |            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/opt/venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 1299, in run_mcp_servers\n",
      "  |     await exit_stack.enter_async_context(mcp_server)\n",
      "  |   File \"/usr/local/lib/python3.13/contextlib.py\", line 668, in enter_async_context\n",
      "  |     result = await _enter(cm)\n",
      "  |              ^^^^^^^^^^^^^^^^\n",
      "  |   File \"/opt/venv/lib/python3.13/site-packages/pydantic_ai/mcp.py\", line 86, in __aenter__\n",
      "  |     self._read_stream, self._write_stream = await self._exit_stack.enter_async_context(self.client_streams())\n",
      "  |                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/usr/local/lib/python3.13/contextlib.py\", line 668, in enter_async_context\n",
      "  |     result = await _enter(cm)\n",
      "  |              ^^^^^^^^^^^^^^^^\n",
      "  |   File \"/usr/local/lib/python3.13/contextlib.py\", line 214, in __aenter__\n",
      "  |     return await anext(self.gen)\n",
      "  |            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/opt/venv/lib/python3.13/site-packages/pydantic_ai/mcp.py\", line 219, in client_streams\n",
      "  |     async with sse_client(\n",
      "  |                ~~~~~~~~~~^\n",
      "  |         url=self.url, headers=self.headers, timeout=self.timeout, sse_read_timeout=self.sse_read_timeout\n",
      "  |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |     ) as (read_stream, write_stream):\n",
      "  |     ^\n",
      "  |   File \"/usr/local/lib/python3.13/contextlib.py\", line 214, in __aenter__\n",
      "  |     return await anext(self.gen)\n",
      "  |            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/opt/venv/lib/python3.13/site-packages/mcp/client/sse.py\", line 43, in sse_client\n",
      "  |     async with anyio.create_task_group() as tg:\n",
      "  |                ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  |   File \"/opt/venv/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 772, in __aexit__\n",
      "  |     raise BaseExceptionGroup(\n",
      "  |         \"unhandled errors in a TaskGroup\", self._exceptions\n",
      "  |     ) from None\n",
      "  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "  +-+---------------- 1 ----------------\n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"/opt/venv/lib/python3.13/site-packages/mcp/client/sse.py\", line 53, in sse_client\n",
      "    |     event_source.response.raise_for_status()\n",
      "    |     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "    |   File \"/opt/venv/lib/python3.13/site-packages/httpx/_models.py\", line 763, in raise_for_status\n",
      "    |     raise HTTPStatusError(message, request=request, response=self)\n",
      "    | httpx.HTTPStatusError: Client error '404 Not Found' for url 'http://localhost:8888/sse'\n",
      "    | For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404\n",
      "    +------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.mcp import MCPServerHTTP\n",
    "\n",
    "server = MCPServerHTTP(url='http://localhost:8888/sse')  \n",
    "agent = Agent(model=ollama_model,\n",
    "              result_type=str,\n",
    "              deps_type=str,\n",
    "              system_prompt=\"\",\n",
    "              mcp_servers=[server],\n",
    "              retries=1)\n",
    "\n",
    "\n",
    "async def main():\n",
    "    async with agent.run_mcp_servers():  \n",
    "        result = await agent.run('How many days between 2000-01-01 and 2025-03-18?')\n",
    "    print(result.data)\n",
    "    #> There are 9,208 days between January 1, 2000, and March 18, 2025.\n",
    "\n",
    "result = await main()\n",
    "print(result.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
