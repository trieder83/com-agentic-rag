{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ff44e18-d9d4-4f3b-a9b4-8ae82ca3879e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://ollama-service-internal.default.svc.cluster.local:11433\n",
      "model: llama3.2:1b\n",
      "embedding: bge-m3:567m\n",
      "datapath /app/data\n",
      "no_proxy None\n",
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "OLLAMA_URL=os.getenv(\"OLLAMA_URL\")\n",
    "LLM_MODEL=os.getenv(\"LLM_MODEL\")\n",
    "EMBEDDING_MODEL=os.getenv(\"EMBEDDING_MODEL\")\n",
    "DATAPATH=os.getenv(\"DATAPATH\")\n",
    "#OLLAMA_URL='http://localhost:11434'\n",
    "LLM_MODEL=\"llama3.2:1b\"\n",
    "#LLM_MODEL=\"llama3.2:3b\" # max retires\n",
    "#LLM_MODEL=\"gemma3:4b\" # no tool support\n",
    "#LLM_MODEL=\"llava:7b\"\n",
    "#LLM_MODEL=\"codellama:7b\"\n",
    "#LLM_MODEL=\"qwen2.5-coder:1.5b\" # no tool support\n",
    "#LLM_MODEL=\"deepseek-r1:1.5b\" # does not support tools\n",
    "#os.setenv(\"no_proxy=localhost,127.0.0.1,*.my-it-solutions.net,ollama\")\n",
    "\n",
    "print(OLLAMA_URL)\n",
    "print(f\"model: {LLM_MODEL}\")\n",
    "print(f\"embedding: {EMBEDDING_MODEL}\")\n",
    "print(f\"datapath {DATAPATH}\")\n",
    "print(f\"no_proxy {os.getenv('no_proxy')}\")\n",
    "\n",
    "import requests\n",
    "import time\n",
    "\n",
    "response = requests.get(OLLAMA_URL,timeout=3)\n",
    "if response.status_code == requests.codes.ok:\n",
    "    print(response)\n",
    "else:\n",
    "    print(response)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28ec15ba-2599-4928-b321-2d9139984960",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from llama_index.llms.ollama import Ollama\n",
    "from pydantic_ai.models.openai import OpenAIModel\n",
    "from pydantic_ai.providers.openai import OpenAIProvider\n",
    "#llm = Ollama(model=LLM_MODEL, request_timeout=120.0, base_url=OLLAMA_URL, temperature=0.0)\n",
    "\n",
    "\n",
    "ollama_model = OpenAIModel(\n",
    "    model_name='LLM_MODEL', provider=OpenAIProvider(base_url=f\"{OLLAMA_URL}/v1\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9bcadbd6-6a2d-4e55-af36-2693f9603691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object start_mcp_server at 0x7ca90c09ac00>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "from starlette.applications import Starlette\n",
    "from starlette.routing import Mount, Host\n",
    "import uvicorn\n",
    "\n",
    "from pydantic_ai import Agent\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "#loop = asyncio.get_event_loop()\n",
    "\n",
    "\n",
    "server = FastMCP('PydanticAI Server')\n",
    "server_agent = Agent(model=ollama_model,\n",
    "              result_type=str,\n",
    "              deps_type=str,\n",
    "              system_prompt=\"\",              \n",
    "              retries=2)\n",
    "\n",
    "@server.tool()\n",
    "async def poet(theme: str) -> str:\n",
    "    \"\"\"Poem generator\"\"\"\n",
    "    print(\"test\")\n",
    "    r = await server_agent.run(f'write a poem about {theme}')    \n",
    "    return r.data\n",
    "\n",
    "@server.prompt()\n",
    "def review_code(code: str) -> str:\n",
    "    return f\"Please review this code:\\n\\n{code}\"    \n",
    "\n",
    "\n",
    "#server.run()\n",
    "#asyncio.run_coroutine_threadsafe(server.run('sse'), loop)\n",
    "#asyncio.run(server.run('sse'))\n",
    "# Mount the SSE server to the existing ASGI server\n",
    "app = Starlette(\n",
    "    routes=[\n",
    "        Mount('/', app=server.sse_app()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# or dynamically mount as host\n",
    "app.router.routes.append(Host('localhost', app=server.sse_app()))\n",
    "\n",
    "#LLM_MODEL=\"llama3.2:3b.run(app, host=\"127.0.0.1\", port=8000)\n",
    "bg = None\n",
    "async def start_mcp_server(bg):\n",
    "    bg = uvicorn.run(app,host=\"localhost\", port=8001)\n",
    "\n",
    "if bg is not None:\n",
    "    bg.stop()\n",
    "    \n",
    "start_mcp_server(bg)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "886b7514-1357-4442-a43f-b2dc2a66cfd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9177f66-b412-4bfb-9214-5f861ea88a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/27/25 23:26:39] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Connecting to SSE endpoint: <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://localhost:8001/sse</span>                        <a href=\"file:///opt/venv/lib/python3.13/site-packages/mcp/client/sse.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">sse.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///opt/venv/lib/python3.13/site-packages/mcp/client/sse.py#45\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">45</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/27/25 23:26:39]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Connecting to SSE endpoint: \u001b[4;94mhttp://localhost:8001/sse\u001b[0m                        \u001b]8;id=157759;file:///opt/venv/lib/python3.13/site-packages/mcp/client/sse.py\u001b\\\u001b[2msse.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=614615;file:///opt/venv/lib/python3.13/site-packages/mcp/client/sse.py#45\u001b\\\u001b[2m45\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  + Exception Group Traceback (most recent call last):\n",
      "  |   File \"/opt/venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py\", line 3547, in run_code\n",
      "  |     await eval(code_obj, self.user_global_ns, self.user_ns)\n",
      "  |   File \"/tmp/ipykernel_1282/3987456570.py\", line 19, in <module>\n",
      "  |     result = await main()\n",
      "  |              ^^^^^^^^^^^^\n",
      "  |   File \"/tmp/ipykernel_1282/3987456570.py\", line 14, in main\n",
      "  |     async with agent.run_mcp_servers():\n",
      "  |                ~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  |   File \"/usr/local/lib/python3.13/contextlib.py\", line 214, in __aenter__\n",
      "  |     return await anext(self.gen)\n",
      "  |            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/opt/venv/lib/python3.13/site-packages/pydantic_ai/agent.py\", line 1299, in run_mcp_servers\n",
      "  |     await exit_stack.enter_async_context(mcp_server)\n",
      "  |   File \"/usr/local/lib/python3.13/contextlib.py\", line 668, in enter_async_context\n",
      "  |     result = await _enter(cm)\n",
      "  |              ^^^^^^^^^^^^^^^^\n",
      "  |   File \"/opt/venv/lib/python3.13/site-packages/pydantic_ai/mcp.py\", line 86, in __aenter__\n",
      "  |     self._read_stream, self._write_stream = await self._exit_stack.enter_async_context(self.client_streams())\n",
      "  |                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/usr/local/lib/python3.13/contextlib.py\", line 668, in enter_async_context\n",
      "  |     result = await _enter(cm)\n",
      "  |              ^^^^^^^^^^^^^^^^\n",
      "  |   File \"/usr/local/lib/python3.13/contextlib.py\", line 214, in __aenter__\n",
      "  |     return await anext(self.gen)\n",
      "  |            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/opt/venv/lib/python3.13/site-packages/pydantic_ai/mcp.py\", line 219, in client_streams\n",
      "  |     async with sse_client(\n",
      "  |                ~~~~~~~~~~^\n",
      "  |         url=self.url, headers=self.headers, timeout=self.timeout, sse_read_timeout=self.sse_read_timeout\n",
      "  |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  |     ) as (read_stream, write_stream):\n",
      "  |     ^\n",
      "  |   File \"/usr/local/lib/python3.13/contextlib.py\", line 214, in __aenter__\n",
      "  |     return await anext(self.gen)\n",
      "  |            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  |   File \"/opt/venv/lib/python3.13/site-packages/mcp/client/sse.py\", line 43, in sse_client\n",
      "  |     async with anyio.create_task_group() as tg:\n",
      "  |                ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  |   File \"/opt/venv/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 772, in __aexit__\n",
      "  |     raise BaseExceptionGroup(\n",
      "  |         \"unhandled errors in a TaskGroup\", self._exceptions\n",
      "  |     ) from None\n",
      "  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "  +-+---------------- 1 ----------------\n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"/opt/venv/lib/python3.13/site-packages/httpx/_transports/default.py\", line 72, in map_httpcore_exceptions\n",
      "    |     yield\n",
      "    |   File \"/opt/venv/lib/python3.13/site-packages/httpx/_transports/default.py\", line 377, in handle_async_request\n",
      "    |     resp = await self._pool.handle_async_request(req)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/opt/venv/lib/python3.13/site-packages/httpcore/_async/connection_pool.py\", line 256, in handle_async_request\n",
      "    |     raise exc from None\n",
      "    |   File \"/opt/venv/lib/python3.13/site-packages/httpcore/_async/connection_pool.py\", line 236, in handle_async_request\n",
      "    |     response = await connection.handle_async_request(\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |         pool_request.request\n",
      "    |         ^^^^^^^^^^^^^^^^^^^^\n",
      "    |     )\n",
      "    |     ^\n",
      "    |   File \"/opt/venv/lib/python3.13/site-packages/httpcore/_async/connection.py\", line 101, in handle_async_request\n",
      "    |     raise exc\n",
      "    |   File \"/opt/venv/lib/python3.13/site-packages/httpcore/_async/connection.py\", line 78, in handle_async_request\n",
      "    |     stream = await self._connect(request)\n",
      "    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/opt/venv/lib/python3.13/site-packages/httpcore/_async/connection.py\", line 124, in _connect\n",
      "    |     stream = await self._network_backend.connect_tcp(**kwargs)\n",
      "    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/opt/venv/lib/python3.13/site-packages/httpcore/_backends/auto.py\", line 31, in connect_tcp\n",
      "    |     return await self._backend.connect_tcp(\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |     ...<5 lines>...\n",
      "    |     )\n",
      "    |     ^\n",
      "    |   File \"/opt/venv/lib/python3.13/site-packages/httpcore/_backends/anyio.py\", line 113, in connect_tcp\n",
      "    |     with map_exceptions(exc_map):\n",
      "    |          ~~~~~~~~~~~~~~^^^^^^^^^\n",
      "    |   File \"/usr/local/lib/python3.13/contextlib.py\", line 162, in __exit__\n",
      "    |     self.gen.throw(value)\n",
      "    |     ~~~~~~~~~~~~~~^^^^^^^\n",
      "    |   File \"/opt/venv/lib/python3.13/site-packages/httpcore/_exceptions.py\", line 14, in map_exceptions\n",
      "    |     raise to_exc(exc) from exc\n",
      "    | httpcore.ConnectError: All connection attempts failed\n",
      "    | \n",
      "    | The above exception was the direct cause of the following exception:\n",
      "    | \n",
      "    | Traceback (most recent call last):\n",
      "    |   File \"/opt/venv/lib/python3.13/site-packages/mcp/client/sse.py\", line 47, in sse_client\n",
      "    |     async with aconnect_sse(\n",
      "    |                ~~~~~~~~~~~~^\n",
      "    |         client,\n",
      "    |         ^^^^^^^\n",
      "    |     ...<2 lines>...\n",
      "    |         timeout=httpx.Timeout(timeout, read=sse_read_timeout),\n",
      "    |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |     ) as event_source:\n",
      "    |     ^\n",
      "    |   File \"/usr/local/lib/python3.13/contextlib.py\", line 214, in __aenter__\n",
      "    |     return await anext(self.gen)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/opt/venv/lib/python3.13/site-packages/httpx_sse/_api.py\", line 69, in aconnect_sse\n",
      "    |     async with client.stream(method, url, headers=headers, **kwargs) as response:\n",
      "    |                ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/usr/local/lib/python3.13/contextlib.py\", line 214, in __aenter__\n",
      "    |     return await anext(self.gen)\n",
      "    |            ^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/opt/venv/lib/python3.13/site-packages/httpx/_client.py\", line 1628, in stream\n",
      "    |     response = await self.send(\n",
      "    |                ^^^^^^^^^^^^^^^^\n",
      "    |     ...<4 lines>...\n",
      "    |     )\n",
      "    |     ^\n",
      "    |   File \"/opt/venv/lib/python3.13/site-packages/httpx/_client.py\", line 1674, in send\n",
      "    |     response = await self._send_handling_auth(\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |     ...<4 lines>...\n",
      "    |     )\n",
      "    |     ^\n",
      "    |   File \"/opt/venv/lib/python3.13/site-packages/httpx/_client.py\", line 1702, in _send_handling_auth\n",
      "    |     response = await self._send_handling_redirects(\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |     ...<3 lines>...\n",
      "    |     )\n",
      "    |     ^\n",
      "    |   File \"/opt/venv/lib/python3.13/site-packages/httpx/_client.py\", line 1739, in _send_handling_redirects\n",
      "    |     response = await self._send_single_request(request)\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/opt/venv/lib/python3.13/site-packages/httpx/_client.py\", line 1776, in _send_single_request\n",
      "    |     response = await transport.handle_async_request(request)\n",
      "    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    |   File \"/opt/venv/lib/python3.13/site-packages/httpx/_transports/default.py\", line 376, in handle_async_request\n",
      "    |     with map_httpcore_exceptions():\n",
      "    |          ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "    |   File \"/usr/local/lib/python3.13/contextlib.py\", line 162, in __exit__\n",
      "    |     self.gen.throw(value)\n",
      "    |     ~~~~~~~~~~~~~~^^^^^^^\n",
      "    |   File \"/opt/venv/lib/python3.13/site-packages/httpx/_transports/default.py\", line 89, in map_httpcore_exceptions\n",
      "    |     raise mapped_exc(message) from exc\n",
      "    | httpx.ConnectError: All connection attempts failed\n",
      "    +------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.mcp import MCPServerHTTP\n",
    "\n",
    "server = MCPServerHTTP(url='http://localhost:8001/sse')  \n",
    "agent = Agent(model=ollama_model,\n",
    "              result_type=str,\n",
    "              deps_type=str,\n",
    "              system_prompt=\"\",\n",
    "              mcp_servers=[server],\n",
    "              retries=1)\n",
    "\n",
    "\n",
    "async def main():\n",
    "    async with agent.run_mcp_servers():  \n",
    "        result = await agent.run('Schreibe ein Gedicht über Füsse')\n",
    "    print(result.data)\n",
    "    #> There are 9,208 days between January 1, 2000, and March 18, 2025.\n",
    "\n",
    "result = await main()\n",
    "print(result.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3ff79d-a208-48b4-a4b5-f48dbbb6b5f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
